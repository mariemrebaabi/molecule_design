{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-01T12:24:35.386264Z","iopub.execute_input":"2023-05-01T12:24:35.386725Z","iopub.status.idle":"2023-05-01T12:24:35.398939Z","shell.execute_reply.started":"2023-05-01T12:24:35.386687Z","shell.execute_reply":"2023-05-01T12:24:35.397684Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"/kaggle/input/testttttt/test11111.txt\n/kaggle/input/train11/train1111.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torchtext==0.6.0","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:24:35.401148Z","iopub.execute_input":"2023-05-01T12:24:35.401924Z","iopub.status.idle":"2023-05-01T12:24:45.376071Z","shell.execute_reply.started":"2023-05-01T12:24:35.401886Z","shell.execute_reply":"2023-05-01T12:24:45.374688Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchtext==0.6.0 in /opt/conda/lib/python3.7/site-packages (0.6.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.6.0) (1.21.6)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchtext==0.6.0) (1.13.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from torchtext==0.6.0) (0.1.97)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.6.0) (4.64.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.6.0) (2.28.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchtext==0.6.0) (1.16.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.6.0) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.6.0) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.6.0) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.6.0) (2.1.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->torchtext==0.6.0) (4.4.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"#batch.py\nimport torch\nfrom torchtext import data\nimport numpy as np\nfrom torch.autograd import Variable\n\n\ndef nopeak_mask(size, opt):\n    np_mask = np.triu(np.ones((1, size, size)), k=1).astype('uint8')\n    if opt.use_cond2dec == True:\n        cond_mask = np.zeros((1, opt.cond_dim, opt.cond_dim))\n        cond_mask_upperright = np.ones((1, opt.cond_dim, size))\n        cond_mask_upperright[:, :, 0] = 0\n        cond_mask_lowerleft = np.zeros((1, size, opt.cond_dim))\n        upper_mask = np.concatenate([cond_mask, cond_mask_upperright], axis=2)\n        lower_mask = np.concatenate([cond_mask_lowerleft, np_mask], axis=2)\n        np_mask = np.concatenate([upper_mask, lower_mask], axis=1)\n    np_mask = Variable(torch.from_numpy(np_mask) == 0)\n    #if opt.device == 0:\n      #np_mask = np_mask.cuda()\n    return np_mask\n\ndef create_masks(src, trg, cond, opt):\n    torch.set_printoptions(profile=\"full\")\n    src_mask = (src != opt.src_pad).unsqueeze(-2)\n    cond_mask = torch.unsqueeze(cond, -2)\n    cond_mask = torch.ones_like(cond_mask, dtype=bool)\n    src_mask = torch.cat([cond_mask, src_mask], dim=2)\n\n    if trg is not None:\n        trg_mask = (trg != opt.trg_pad).unsqueeze(-2)\n        if opt.use_cond2dec == True:\n            trg_mask = torch.cat([cond_mask, trg_mask], dim=2)\n        np_mask = nopeak_mask(trg.size(1), opt)\n        #if trg.is_cuda:\n            #np_mask.cuda()\n        trg_mask = trg_mask & np_mask\n\n    else:\n        trg_mask = None\n    return src_mask, trg_mask\n\n# patch on Torchtext's batching process that makes it more efficient\n# from http://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks\n\nclass MyIterator(data.Iterator):\n    def create_batches(self):\n        if self.train:\n            def pool(d, random_shuffler):\n                for p in data.batch(d, self.batch_size * 100):\n                    p_batch = data.batch(sorted(p, key=self.sort_key), self.batch_size, self.batch_size_fn)\n                    for b in random_shuffler(list(p_batch)):\n                        yield b\n            self.batches = pool(self.data(), self.random_shuffler)\n            \n        else:\n            self.batches = []\n            for b in data.batch(self.data(), self.batch_size,\n                                          self.batch_size_fn):\n                self.batches.append(sorted(b, key=self.sort_key))\n\nglobal max_src_in_batch, max_tgt_in_batch\n\ndef batch_size_fn(new, count, sofar):\n    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n    global max_src_in_batch, max_tgt_in_batch\n    if count == 1:\n        max_src_in_batch = 0\n        max_tgt_in_batch = 0\n    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n    src_elements = count * max_src_in_batch\n    tgt_elements = count * max_tgt_in_batch\n    return max(src_elements, tgt_elements)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:24:45.380947Z","iopub.execute_input":"2023-05-01T12:24:45.381296Z","iopub.status.idle":"2023-05-01T12:24:45.401883Z","shell.execute_reply.started":"2023-05-01T12:24:45.381263Z","shell.execute_reply":"2023-05-01T12:24:45.400580Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#beam\nimport torch\n#from Batch import nopeak_mask\nimport torch.nn.functional as F\nimport math\nimport numpy as np\n\n\ndef init_vars(cond, model, SRC, TRG, toklen, opt, z):\n    init_tok = TRG.vocab.stoi['<sos>']\n\n    src_mask = (torch.ones(1, 1, toklen) != 0)\n    trg_mask = nopeak_mask(1, opt)\n\n    trg_in = torch.LongTensor([[init_tok]])\n\n\n    if opt.device == 0:\n        #trg_in, z, src_mask, trg_mask = trg_in.cuda(), z.cuda(), src_mask.cuda(), trg_mask.cuda()\n        trg_in, z, src_mask, trg_mask = trg_in, z, src_mask, trg_mask\n\n\n    if opt.use_cond2dec == True:\n        output_mol = model.out(model.decoder(trg_in, z, cond, src_mask, trg_mask))[:, 3:, :]\n    else:\n        output_mol = model.out(model.decoder(trg_in, z, cond, src_mask, trg_mask))\n    out_mol = F.softmax(output_mol, dim=-1)\n    \n    probs, ix = out_mol[:, -1].data.topk(opt.k)\n    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n    \n    outputs = torch.zeros(opt.k, opt.max_strlen).long()\n    if opt.device == 0:\n        outputs = outputs#.cuda()\n    outputs[:, 0] = init_tok\n    outputs[:, 1] = ix[0]\n\n    e_outputs = torch.zeros(opt.k, z.size(-2), z.size(-1))\n    if opt.device == 0:\n        e_outputs = e_outputs#.cuda()\n    e_outputs[:, :] = z[0]\n    \n    return outputs, e_outputs, log_scores\n\ndef k_best_outputs(outputs, out, log_scores, i, k):\n    probs, ix = out[:, -1].data.topk(k)\n    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0,1)\n    k_probs, k_ix = log_probs.view(-1).topk(k)\n    \n    row = k_ix // k\n    col = k_ix % k\n\n    outputs[:, :i] = outputs[row, :i]\n    outputs[:, i] = ix[row, col]\n\n    log_scores = k_probs.unsqueeze(0)\n    \n    return outputs, log_scores\n\ndef beam_search(cond, model, SRC, TRG, toklen, opt, z):\n    if opt.device == 0:\n        #cond = cond.cuda()\n        cond = cond\n    cond = cond.view(1, -1)\n\n    outputs, e_outputs, log_scores = init_vars(cond, model, SRC, TRG, toklen, opt, z)\n    cond = cond.repeat(opt.k, 1)\n    src_mask = (torch.ones(1, 1, toklen) != 0)\n    src_mask = src_mask.repeat(opt.k, 1, 1)\n    #if opt.device == 0:\n        #src_mask = src_mask.cuda()\n    eos_tok = TRG.vocab.stoi['<eos>']\n\n    ind = None\n    for i in range(2, opt.max_strlen):\n        trg_mask = nopeak_mask(i, opt)\n        trg_mask = trg_mask.repeat(opt.k, 1, 1)\n\n        if opt.use_cond2dec == True:\n            output_mol = model.out(model.decoder(outputs[:,:i], e_outputs, cond, src_mask, trg_mask))[:, 3:, :]\n        else:\n            output_mol = model.out(model.decoder(outputs[:,:i], e_outputs, cond, src_mask, trg_mask))\n        out_mol = F.softmax(output_mol, dim=-1)\n    \n        outputs, log_scores = k_best_outputs(outputs, out_mol, log_scores, i, opt.k)\n        ones = (outputs==eos_tok).nonzero() # Occurrences of end symbols for all input sentences.\n        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long)#.cuda()\n        for vec in ones:\n            i = vec[0]\n            if sentence_lengths[i]==0: # First end symbol has not been found yet\n                sentence_lengths[i] = vec[1] # Position of first end symbol\n\n        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\n\n        if num_finished_sentences == opt.k:\n            alpha = 0.7\n            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n            _, ind = torch.max(log_scores * div, 1)\n            ind = ind.data[0]\n            break\n    \n    if ind is None:\n        length = (outputs[0]==eos_tok).nonzero()[0]\n        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n    \n    else:\n        length = (outputs[ind]==eos_tok).nonzero()[0]\n        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:24:45.403936Z","iopub.execute_input":"2023-05-01T12:24:45.404849Z","iopub.status.idle":"2023-05-01T12:24:45.432848Z","shell.execute_reply.started":"2023-05-01T12:24:45.404807Z","shell.execute_reply":"2023-05-01T12:24:45.431564Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#embed.py\nimport torch\nimport torch.nn as nn\nimport math\nfrom torch.autograd import Variable\n\n\nclass Embedder(nn.Module):\n    def __init__(self, vocab_size, d_model):\n        super().__init__()\n        self.d_model = d_model\n        self.embed = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, x):\n        return self.embed(x)\n\n\nclass PositionalEncoder(nn.Module):\n    def __init__(self, d_model, max_seq_len=200, dropout=0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.dropout = nn.Dropout(dropout)\n        # create constant 'pe' matrix with values dependant on \n        # pos and i\n        pe = torch.zeros(max_seq_len, d_model)\n        for pos in range(max_seq_len):\n            for i in range(0, d_model, 2):\n                pe[pos, i] = \\\n                    math.sin(pos / (10000 ** ((2 * i) / d_model)))\n                pe[pos, i + 1] = \\\n                    math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # make embeddings relatively larger\n        x = x * math.sqrt(self.d_model)\n        # add constant to embedding\n        seq_len = x.size(1)\n        pe = Variable(self.pe[:, :seq_len], requires_grad=False)\n        if x.is_cuda:\n            pe.cuda()\n        x = x + pe\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:24:45.435491Z","iopub.execute_input":"2023-05-01T12:24:45.435857Z","iopub.status.idle":"2023-05-01T12:24:45.450423Z","shell.execute_reply.started":"2023-05-01T12:24:45.435826Z","shell.execute_reply":"2023-05-01T12:24:45.449399Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"#sublayer\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport numpy as np\n\n\nclass Norm(nn.Module):\n    def __init__(self, d_model, eps=1e-6):\n        super().__init__()\n\n        self.size = d_model\n\n        # create two learnable parameters to calibrate normalisation\n        self.alpha = nn.Parameter(torch.ones(self.size))\n        self.bias = nn.Parameter(torch.zeros(self.size))\n\n        self.eps = eps\n\n    def forward(self, x):\n        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n               / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n        return norm\n\n\ndef attention(q, k, v, d_k, mask=None, dropout=None):\n    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n\n    if mask is not None:\n        mask = mask.unsqueeze(1)\n        scores = scores.masked_fill(mask == 0, -1e9)\n\n    scores = F.softmax(scores, dim=-1)\n\n    if dropout is not None:\n        scores = dropout(scores)\n\n    output = torch.matmul(scores, v)\n    return output\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, heads, d_model, dropout=0.1):\n        super().__init__()\n\n        self.d_model = d_model\n        self.d_k = d_model // heads\n        self.h = heads\n\n        self.q_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n\n        self.dropout = nn.Dropout(dropout)\n        self.out = nn.Linear(d_model, d_model)\n\n    def forward(self, q, k, v, mask=None):\n        bs = q.size(0)\n\n        # perform linear operation and split into N heads\n        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n\n        # transpose to get dimensions bs * N * sl * d_model\n        k = k.transpose(1, 2)\n        q = q.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        # calculate attention using function we will define next\n        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n        # concatenate heads and put through final linear layer\n        concat = scores.transpose(1, 2).contiguous() \\\n            .view(bs, -1, self.d_model)\n        output = self.out(concat)\n\n        return output\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n        super().__init__()\n\n        # We set d_ff as a default to 2048\n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n\n    def forward(self, x):\n        x = self.dropout(F.gelu(self.linear_1(x)))\n        x = self.linear_2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:24:45.453797Z","iopub.execute_input":"2023-05-01T12:24:45.454070Z","iopub.status.idle":"2023-05-01T12:24:45.473326Z","shell.execute_reply.started":"2023-05-01T12:24:45.454044Z","shell.execute_reply":"2023-05-01T12:24:45.472392Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#layer.py\nimport torch\nimport torch.nn as nn\n#from Sublayers import FeedForward, MultiHeadAttention, Norm\nimport numpy as np\n\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, heads, dropout=0.1):\n        super().__init__()\n        self.norm_1 = Norm(d_model)\n        self.norm_2 = Norm(d_model)\n        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n        self.ff = FeedForward(d_model, dropout=dropout)\n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        x2 = self.norm_1(x)\n        x = x + self.dropout_1(self.attn(x2, x2, x2, mask))\n        x2 = self.norm_2(x)\n        x = x + self.dropout_2(self.ff(x2))\n        return x\n    \n# build a decoder layer with two multi-head attention layers and\n# one feed-forward layer\nclass DecoderLayer(nn.Module):\n    def __init__(self, opt, d_model, heads, dropout=0.1):\n        super().__init__()\n        self.use_cond2dec = opt.use_cond2dec\n        self.use_cond2lat = opt.use_cond2lat\n        self.norm_1 = Norm(d_model)\n        self.norm_2 = Norm(d_model)\n        self.norm_3 = Norm(d_model)\n\n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        self.dropout_3 = nn.Dropout(dropout)\n\n        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n        self.ff = FeedForward(d_model, dropout=dropout)\n\n    def forward(self, x, e_outputs, cond_input, src_mask, trg_mask):\n        x2 = self.norm_1(x)\n        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n        x2 = self.norm_2(x)\n        if self.use_cond2lat == True:\n            cond_mask = torch.unsqueeze(cond_input, -2)\n            cond_mask = torch.ones_like(cond_mask, dtype=bool)\n            src_mask = torch.cat([cond_mask, src_mask], dim=2)\n\n        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n        x2 = self.norm_3(x)\n        x = x + self.dropout_3(self.ff(x2))\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:24:45.475599Z","iopub.execute_input":"2023-05-01T12:24:45.476067Z","iopub.status.idle":"2023-05-01T12:24:45.492247Z","shell.execute_reply.started":"2023-05-01T12:24:45.476028Z","shell.execute_reply":"2023-05-01T12:24:45.491267Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"!pip install rdkit","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:24:45.493748Z","iopub.execute_input":"2023-05-01T12:24:45.494296Z","iopub.status.idle":"2023-05-01T12:24:55.480175Z","shell.execute_reply.started":"2023-05-01T12:24:45.494259Z","shell.execute_reply":"2023-05-01T12:24:55.478918Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Requirement already satisfied: rdkit in /opt/conda/lib/python3.7/site-packages (2022.9.5)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from rdkit) (9.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rdkit) (1.21.6)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"#calcul proprties \nimport sys\nimport numpy as np\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors, QED\nimport pandas as pd\n\n\ndef printProgressBar(i,max,postText):\n    n_bar = 20 #size of progress bar\n    j= i/max\n    sys.stdout.write('\\r')\n    sys.stdout.write(f\"  [{'#' * int(n_bar * j):{n_bar}s}] {int(100 * j)}%  {postText}\")\n    sys.stdout.flush()\n\ndef calcProperty(opt):\n    data = [opt.src_data, opt.src_data_te]\n    for data_kind in data:\n        if data_kind == opt.src_data:\n            print(\"Calculating properties for {} train molecules: logP, tPSA, QED\".format(len(opt.src_data)))\n        if data_kind == opt.src_data_te:\n            print(\"Calculating properties for {} test molecules: logP, tPSA, QED\".format(len(opt.src_data_te)))\n        count = 0\n        mol_list, logP_list, tPSA_list, QED_list = [], [], [], []\n\n        for smi in opt.src_data:\n            count += 1\n            printProgressBar(int(count / len(opt.src_data) * 100), 100, 'completed!')\n            mol = Chem.MolFromSmiles(smi)\n            mol_list.append(smi), logP_list.append(Descriptors.MolLogP(mol)), tPSA_list.append(Descriptors.TPSA(mol)), QED_list.append(QED.qed(mol))\n\n        if data_kind == opt.src_data:\n            prop_df = pd.DataFrame({'logP': logP_list, 'tPSA': tPSA_list, 'QED': QED_list})\n            prop_df.to_csv(\"/kaggle/working/prop_temp.csv\", index=False)\n        if data_kind == opt.src_data_te:\n            prop_df_te = pd.DataFrame({'logP': logP_list, 'tPSA': tPSA_list, 'QED': QED_list})\n            prop_df_te.to_csv(\"/kaggle/working/prop_temp_te.csv\", index=False)\n\n    return prop_df, prop_df_te","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:24:55.483973Z","iopub.execute_input":"2023-05-01T12:24:55.484358Z","iopub.status.idle":"2023-05-01T12:24:55.496891Z","shell.execute_reply.started":"2023-05-01T12:24:55.484323Z","shell.execute_reply":"2023-05-01T12:24:55.495560Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"!pip install  selfies==1.0.4\n!pip install smilespe==0.0.3","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:24:55.499117Z","iopub.execute_input":"2023-05-01T12:24:55.499482Z","iopub.status.idle":"2023-05-01T12:25:16.012471Z","shell.execute_reply.started":"2023-05-01T12:24:55.499446Z","shell.execute_reply":"2023-05-01T12:25:16.010795Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Requirement already satisfied: selfies==1.0.4 in /opt/conda/lib/python3.7/site-packages (1.0.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: smilespe==0.0.3 in /opt/conda/lib/python3.7/site-packages (0.0.3)\nRequirement already satisfied: fastprogress in /opt/conda/lib/python3.7/site-packages (from smilespe==0.0.3) (1.0.3)\nRequirement already satisfied: gensim in /opt/conda/lib/python3.7/site-packages (from smilespe==0.0.3) (4.2.0)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim->smilespe==0.0.3) (6.3.0)\nRequirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim->smilespe==0.0.3) (1.7.3)\nRequirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.7/site-packages (from gensim->smilespe==0.0.3) (1.21.6)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"#tpkonize\nimport selfies\nfrom SmilesPE.pretokenizer import atomwise_tokenizer, kmer_tokenizer\nimport spacy\nimport re\n\n\nclass moltokenize(object):\n    def tokenizer(self, sentence):\n        return [tok for tok in atomwise_tokenizer(sentence) if tok != \" \"]","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:25:16.017577Z","iopub.execute_input":"2023-05-01T12:25:16.018833Z","iopub.status.idle":"2023-05-01T12:25:16.025751Z","shell.execute_reply.started":"2023-05-01T12:25:16.018780Z","shell.execute_reply":"2023-05-01T12:25:16.024526Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"!pip install dill==0.3.4","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:25:16.027436Z","iopub.execute_input":"2023-05-01T12:25:16.028324Z","iopub.status.idle":"2023-05-01T12:25:26.091657Z","shell.execute_reply.started":"2023-05-01T12:25:16.028283Z","shell.execute_reply":"2023-05-01T12:25:26.090359Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Requirement already satisfied: dill==0.3.4 in /opt/conda/lib/python3.7/site-packages (0.3.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"#process.py\nimport pandas as pd\nimport torch\nimport torchtext\nfrom torchtext import data\n#from Tokenize import moltokenize\n#from Batch import MyIterator, batch_size_fn\nimport os\n#import dill as pickle\nimport pickle\nimport numpy as np\n\n\ndef read_data(opt):\n    if opt.src_data is not None:\n        try:\n            opt.src_data = open(opt.src_data, 'rt', encoding='UTF8').read().strip().split('\\n')\n        except:\n            print(\"error: '\" + opt.src_data + \"' file not found\")\n            quit()\n\n    if opt.trg_data is not None:\n        try:\n            opt.trg_data = open(opt.trg_data, 'rt', encoding='UTF8').read().strip().split('\\n')\n        except:\n            print(\"error: '\" + opt.trg_data + \"' file not found\")\n            quit()\n\n    if opt.src_data_te is not None:\n        try:\n            opt.src_data_te = open(opt.src_data_te, 'rt', encoding='UTF8').read().strip().split('\\n')\n        except:\n            print(\"error: '\" + opt.src_data_te + \"' file not found\")\n            quit()\n\n    if opt.trg_data_te is not None:\n        try:\n            opt.trg_data_te = open(opt.trg_data_te, 'rt', encoding='UTF8').read().strip().split('\\n')\n        except:\n            print(\"error: '\" + opt.trg_data_te + \"' file not found\")\n            quit()\n\n\ndef create_fields(opt):\n    lang_formats = ['SMILES', 'SELFIES']\n    if opt.lang_format not in lang_formats:\n        print('invalid src language: ' + opt.lang_forma + 'supported languages : ' + lang_formats)\n\n    print(\"loading molecule tokenizers...\")\n\n    t_src = moltokenize()\n    t_trg = moltokenize()\n\n    SRC = data.Field(tokenize=t_src.tokenizer)\n    TRG = data.Field(tokenize=t_trg.tokenizer, init_token='<sos>', eos_token='<eos>')\n\n    if opt.load_weights is not None:\n        try:\n            print(\"loading presaved fields...\")\n            SRC = pickle.load(open(f'{opt.load_weights}/SRC.pkl', 'rb'))\n            TRG = pickle.load(open(f'{opt.load_weights}/TRG.pkl', 'rb'))\n\n        except:\n            print(\"error opening SRC.pkl and TXT.pkl field files, please ensure they are in \" + opt.load_weights + \"/\")\n            quit()\n\n    return (SRC, TRG)\n\n\ndef create_dataset(opt, SRC, TRG, PROP, tr_te):\n    # masking data longer than max_strlen\n    if tr_te == \"tr\":\n        print(\"\\n* creating [train] dataset and iterator... \")\n        raw_data = {'src': [line for line in opt.src_data], 'trg': [line for line in opt.trg_data]}\n    if tr_te == \"te\":\n        print(\"\\n* creating [test] dataset and iterator... \")\n        raw_data = {'src': [line for line in opt.src_data_te], 'trg': [line for line in opt.trg_data_te]}\n    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n    df = pd.concat([df, PROP], axis=1)\n\n    # if tr_te == \"tr\":  #for code test\n    #     df = df[:30000]\n    # if tr_te == \"te\":\n    #     df = df[:3000]\n\n    if opt.lang_format == 'SMILES':\n        mask = (df['src'].str.len() + opt.cond_dim < opt.max_strlen) & (df['trg'].str.len() + opt.cond_dim < opt.max_strlen)\n    # if opt.lang_format == 'SELFIES':\n    #     mask = (df['src'].str.count('][') + opt.cond_dim < opt.max_strlen) & (df['trg'].str.count('][') + opt.cond_dim < opt.max_strlen)\n\n    df = df.loc[mask]\n    if tr_te == \"tr\":\n        print(\"     - # of training samples:\", len(df.index))\n        df.to_csv(\"DB_transformer_temp.csv\", index=False)\n    if tr_te == \"te\":\n        print(\"     - # of test samples:\", len(df.index))\n        df.to_csv(\"DB_transformer_temp_te.csv\", index=False)\n\n    logP = data.Field(use_vocab=False, sequential=False, dtype=torch.float)\n    tPSA = data.Field(use_vocab=False, sequential=False, dtype=torch.float)\n    QED = data.Field(use_vocab=False, sequential=False, dtype=torch.float)\n\n    data_fields = [('src', SRC), ('trg', TRG), ('logP', logP), ('tPSA', tPSA), ('QED', QED)]\n\n    if tr_te == \"tr\":\n        toklenList = []\n        train = data.TabularDataset('./DB_transformer_temp.csv', format='csv', fields=data_fields, skip_header=True)\n        for i in range(len(train)):\n            toklenList.append(len(vars(train[i])['src']))\n        df_toklenList = pd.DataFrame(toklenList, columns=[\"toklen\"])\n        df_toklenList.to_csv(\"toklen_list.csv\", index=False)\n        if opt.verbose == True:\n            print(\"     - tokenized training sample 0:\", vars(train[0]))\n    if tr_te == \"te\":\n        train = data.TabularDataset('./DB_transformer_temp_te.csv', format='csv', fields=data_fields, skip_header=True)\n        if opt.verbose == True:\n            print(\"     - tokenized testing sample 0:\", vars(train[0]))\n\n    train_iter = MyIterator(train, batch_size=opt.batchsize, device=opt.device,\n                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg), len(x.logP), len(x.tPSA), len(x.QED)),\n                            batch_size_fn=batch_size_fn, train=True, shuffle=True)\n    try:\n        os.remove('DB_transformer_temp.csv')\n    except:\n        pass\n    try:\n        os.remove('DB_transformer_temp_te.csv')\n    except:\n        pass\n\n    if tr_te == \"tr\":\n        if opt.load_weights is None:\n            print(\"     - building vocab from train data...\")\n            SRC.build_vocab(train)\n            if opt.verbose == True:\n                print('     - vocab size of SRC: {}\\n        -> {}'.format(len(SRC.vocab), SRC.vocab.stoi))\n            TRG.build_vocab(train)\n            if opt.verbose == True:\n                print('     - vocab size of TRG: {}\\n        -> {}'.format(len(TRG.vocab), TRG.vocab.stoi))\n            if opt.checkpoint > 0:\n                try:\n                    os.mkdir(\"weights\")\n                except:\n                    print(\"weights folder already exists, run program with -load_weights weights to load them\")\n                    quit()\n                pickle.dump(SRC, open('weights/SRC.pkl', 'wb'))\n                pickle.dump(TRG, open('weights/TRG.pkl', 'wb'))\n\n        opt.src_pad = SRC.vocab.stoi['<pad>']\n        opt.trg_pad = TRG.vocab.stoi['<pad>']\n\n        opt.train_len = get_len(train_iter)\n\n    if tr_te == \"te\":\n        opt.test_len = get_len(train_iter)\n\n    return train_iter\n\n\ndef get_len(train):\n    for i, b in enumerate(train):\n        pass\n    return i\n","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:25:26.093898Z","iopub.execute_input":"2023-05-01T12:25:26.094579Z","iopub.status.idle":"2023-05-01T12:25:26.123880Z","shell.execute_reply.started":"2023-05-01T12:25:26.094536Z","shell.execute_reply":"2023-05-01T12:25:26.122665Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"#embed\nimport torch\nimport torch.nn as nn\nimport math\nfrom torch.autograd import Variable\n\n\nclass Embedder(nn.Module):\n    def __init__(self, vocab_size, d_model):\n        super().__init__()\n        self.d_model = d_model\n        self.embed = nn.Embedding(vocab_size, d_model)\n\n    def forward(self, x):\n        return self.embed(x)\n\n\nclass PositionalEncoder(nn.Module):\n    def __init__(self, d_model, max_seq_len=200, dropout=0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.dropout = nn.Dropout(dropout)\n        # create constant 'pe' matrix with values dependant on \n        # pos and i\n        pe = torch.zeros(max_seq_len, d_model)\n        for pos in range(max_seq_len):\n            for i in range(0, d_model, 2):\n                pe[pos, i] = \\\n                    math.sin(pos / (10000 ** ((2 * i) / d_model)))\n                pe[pos, i + 1] = \\\n                    math.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        # make embeddings relatively larger\n        x = x * math.sqrt(self.d_model)\n        # add constant to embedding\n        seq_len = x.size(1)\n        pe = Variable(self.pe[:, :seq_len], requires_grad=False)\n        #if x.is_cuda:\n            #pe.cuda()\n        x = x + pe\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:25:26.124973Z","iopub.execute_input":"2023-05-01T12:25:26.125939Z","iopub.status.idle":"2023-05-01T12:25:26.143643Z","shell.execute_reply.started":"2023-05-01T12:25:26.125897Z","shell.execute_reply":"2023-05-01T12:25:26.142556Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#model\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n#from Layers import EncoderLayer, DecoderLayer\n#from Embed import Embedder, PositionalEncoder\n#from Sublayers import Norm\nimport copy\nimport numpy as np\n\ndef get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n\nclass Encoder(nn.Module):\n    def __init__(self, opt, vocab_size, d_model, N, heads, dropout):\n        super().__init__()\n        self.N = N\n        self.cond_dim = opt.cond_dim\n        self.d_model = d_model\n        self.embed_sentence = Embedder(vocab_size, d_model)\n        self.embed_cond2enc = nn.Linear(opt.cond_dim, d_model*opt.cond_dim)\n        self.pe = PositionalEncoder(d_model, dropout=dropout)\n        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n        self.norm = Norm(d_model)\n\n        self.fc_mu = nn.Linear(d_model, opt.latent_dim)\n        self.fc_log_var = nn.Linear(d_model, opt.latent_dim)\n\n    def forward(self, src, cond_input, mask):\n        cond2enc = self.embed_cond2enc(cond_input).view(cond_input.size(0), cond_input.size(1), -1)\n        x = self.embed_sentence(src)\n        x = torch.cat([cond2enc, x], dim=1)\n        x = self.pe(x)\n        for i in range(self.N):\n            x = self.layers[i](x, mask)\n        x = self.norm(x)\n\n        mu = self.fc_mu(x)\n        log_var = self.fc_log_var(x)\n        return self.sampling(mu, log_var), mu, log_var\n\n    def sampling(self, mu, log_var):\n        std = torch.exp(0.5*log_var)\n        eps = torch.randn_like(std)\n        return eps.mul(std).add(mu)\n    \nclass Decoder(nn.Module):\n    def __init__(self, opt, vocab_size, d_model, N, heads, dropout):\n        super().__init__()\n        self.N = N\n        self.cond_dim = opt.cond_dim\n        self.d_model = d_model\n        self.use_cond2dec = opt.use_cond2dec\n        self.use_cond2lat = opt.use_cond2lat\n        self.embed = Embedder(vocab_size, d_model)\n        if self.use_cond2dec == True:\n            self.embed_cond2dec = nn.Linear(opt.cond_dim, d_model * opt.cond_dim) #concat to trg_input\n        if self.use_cond2lat == True:\n            self.embed_cond2lat = nn.Linear(opt.cond_dim, d_model * opt.cond_dim) #concat to trg_input\n        self.pe = PositionalEncoder(d_model, dropout=dropout)\n        self.fc_z = nn.Linear(opt.latent_dim, d_model)\n        self.layers = get_clones(DecoderLayer(opt, d_model, heads, dropout), N)\n        self.norm = Norm(d_model)\n\n    def forward(self, trg, e_outputs, cond_input, src_mask, trg_mask):\n        x = self.embed(trg)\n        e_outputs = self.fc_z(e_outputs)\n        if self.use_cond2dec == True:\n            cond2dec = self.embed_cond2dec(cond_input).view(cond_input.size(0), cond_input.size(1), -1)\n            x = torch.cat([cond2dec, x], dim=1)\n        if self.use_cond2lat == True:\n            cond2lat = self.embed_cond2lat(cond_input).view(cond_input.size(0), cond_input.size(1), -1)\n            e_outputs = torch.cat([cond2lat, e_outputs], dim=1)\n        x = self.pe(x)\n        for i in range(self.N):\n            x = self.layers[i](x, e_outputs, cond_input, src_mask, trg_mask)\n        return self.norm(x)\n\nclass Transformer(nn.Module):\n    def __init__(self, opt, src_vocab, trg_vocab):\n        super().__init__()\n        self.use_cond2dec = opt.use_cond2dec\n        self.use_cond2lat = opt.use_cond2lat\n        self.encoder = Encoder(opt, src_vocab, opt.d_model, opt.n_layers, opt.heads, opt.dropout)\n        self.decoder = Decoder(opt, trg_vocab, opt.d_model, opt.n_layers, opt.heads, opt.dropout)\n        self.out = nn.Linear(opt.d_model, trg_vocab)\n        if self.use_cond2dec == True:\n            self.prop_fc = nn.Linear(trg_vocab, 1)\n    def forward(self, src, trg, cond, src_mask, trg_mask):\n        z, mu, log_var = self.encoder(src, cond, src_mask)\n        d_output = self.decoder(trg, z, cond, src_mask, trg_mask)\n        output = self.out(d_output)\n        if self.use_cond2dec == True:\n            output_prop, output_mol = self.prop_fc(output[:, :3, :]), output[:, 3:, :]\n        else:\n            output_prop, output_mol = torch.zeros(output.size(0), 3, 1), output\n        return output_prop, output_mol, mu, log_var, z\n\ndef get_model(opt, src_vocab, trg_vocab):\n    assert opt.d_model % opt.heads == 0\n    assert opt.dropout < 1\n\n    model = Transformer(opt, src_vocab, trg_vocab)\n    if opt.print_model == True:\n        print(\"model structure:\\n\", model)\n\n    if opt.load_weights is not None:\n        print(\"loading pretrained weights...\")\n        model.load_state_dict(torch.load(f'{opt.load_weights}/model_weights'))\n    else:\n        for p in model.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n\n    #if opt.device == 0:\n        #model = model.cuda()\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:25:26.145150Z","iopub.execute_input":"2023-05-01T12:25:26.145583Z","iopub.status.idle":"2023-05-01T12:25:26.173438Z","shell.execute_reply.started":"2023-05-01T12:25:26.145545Z","shell.execute_reply":"2023-05-01T12:25:26.172329Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"#optimize\nimport torch\nimport numpy as np\n# code from AllenNLP\n\nclass CosineWithRestarts(torch.optim.lr_scheduler._LRScheduler):\n    \"\"\"\n    Cosine annealing with restarts.\n\n    Parameters\n    ----------\n    optimizer : torch.optim.Optimizer\n\n    T_max : int\n        The maximum number of iterations within the first cycle.\n\n    eta_min : float, optional (default: 0)\n        The minimum learning rate.\n\n    last_epoch : int, optional (default: -1)\n        The index of the last epoch.\n\n    \"\"\"\n\n    def __init__(self,\n                 optimizer: torch.optim.Optimizer,\n                 T_max: int,\n                 eta_min: float = 0.,\n                 last_epoch: int = -1,\n                 factor: float = 1.) -> None:\n        # pylint: disable=invalid-name\n        self.T_max = T_max\n        self.eta_min = eta_min\n        self.factor = factor\n        self._last_restart: int = 0\n        self._cycle_counter: int = 0\n        self._cycle_factor: float = 1.\n        self._updated_cycle_len: int = T_max\n        self._initialized: bool = False\n        super(CosineWithRestarts, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        \"\"\"Get updated learning rate.\"\"\"\n        # HACK: We need to check if this is the first time get_lr() was called, since\n        # we want to start with step = 0, but _LRScheduler calls get_lr with\n        # last_epoch + 1 when initialized.\n        if not self._initialized:\n            self._initialized = True\n            return self.base_lrs\n\n        # step = self.last_epoch + 1\n        step = self.last_epoch\n        self._cycle_counter = step - self._last_restart\n\n        lrs = [\n            (\n                self.eta_min + ((lr - self.eta_min) / 2) *\n                (\n                    np.cos(\n                        np.pi *\n                        ((self._cycle_counter) % self._updated_cycle_len) /\n                        self._updated_cycle_len\n                    ) + 1\n                )\n            ) for lr in self.base_lrs\n        ]\n\n        if self._cycle_counter % self._updated_cycle_len == 0:\n            # Adjust the cycle length.\n            self._cycle_factor *= self.factor\n            self._cycle_counter = 0\n            self._updated_cycle_len = int(self._cycle_factor * self.T_max)\n            self._last_restart = step\n\n        return lrs\n\n\nclass WarmUpDefault(torch.optim.lr_scheduler._LRScheduler):\n    \"\"\"\n    Cosine annealing with restarts.\n\n    Parameters\n    ----------\n    optimizer : torch.optim.Optimizer\n\n    T_max : int\n        The maximum number of iterations within the first cycle.\n\n    eta_min : float, optional (default: 0)\n        The minimum learning rate.\n\n    last_epoch : int, optional (default: -1)\n        The index of the last epoch.\n\n    \"\"\"\n\n    def __init__(self,\n                 optimizer: torch.optim.Optimizer,\n                 T_max: int,\n                 eta_min: float = 0.,\n                 last_epoch: int = -1,\n                 factor: float = 1.) -> None:\n        # pylint: disable=invalid-name\n        self.T_max = T_max\n        self.eta_min = eta_min\n        self.factor = factor\n        self._last_restart: int = 0\n        self._cycle_counter: int = 0\n        self._cycle_factor: float = 1.\n        self._updated_cycle_len: int = T_max\n        self._initialized: bool = False\n        super(CosineWithRestarts, self).__init__(optimizer, last_epoch)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:25:26.175749Z","iopub.execute_input":"2023-05-01T12:25:26.176559Z","iopub.status.idle":"2023-05-01T12:25:26.191661Z","shell.execute_reply.started":"2023-05-01T12:25:26.176522Z","shell.execute_reply":"2023-05-01T12:25:26.190574Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"%config NotebookApp.iopub_msg_rate_limit=1000000\n","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:25:26.193325Z","iopub.execute_input":"2023-05-01T12:25:26.193756Z","iopub.status.idle":"2023-05-01T12:25:26.212323Z","shell.execute_reply.started":"2023-05-01T12:25:26.193721Z","shell.execute_reply":"2023-05-01T12:25:26.211323Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"#train\nimport argparse\nimport time\nimport torch\nimport numpy as np\n#from Models import get_model\n#from Process import *\nimport torch.nn.functional as F\n#from Optim import CosineWithRestarts\n#from Batch import create_masks\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nimport joblib\n#import dill as pickle\nimport pickle\nimport pandas as pd\n#from calProp import calcProperty\nimport csv\nimport timeit\n\ndef KLAnnealer(opt, epoch):\n    beta = opt.KLA_ini_beta + opt.KLA_inc_beta * ((epoch + 1) - opt.KLA_beg_epoch)\n    return beta\n\ndef loss_function(opt, beta, preds_prop, preds_mol, ys_cond, ys_mol, mu, log_var):\n    RCE_mol = F.cross_entropy(preds_mol.contiguous().view(-1, preds_mol.size(-1)), ys_mol, ignore_index=opt.trg_pad, reduction='sum')\n    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n    if opt.use_cond2dec == True:\n        RCE_prop = F.mse_loss(preds_prop, ys_cond, reduction='sum')\n        loss = RCE_mol + RCE_prop + beta * KLD\n    else:\n        RCE_prop = torch.zeros(1)\n        loss = RCE_mol + beta * KLD\n    return loss, RCE_mol, RCE_prop, KLD\n\ndef train_model(model, opt):\n    print(\"training model...\")\n    global robustScaler\n    model.train()\n\n    start = time.time()\n    if opt.checkpoint > 0:\n        cptime = time.time()\n\n    history_epoch, history_beta, history_lr = [], [], []\n    history_total_loss, history_RCE_mol_loss, history_RCE_prop_loss, history_KLD_loss = [], [], [], []\n    history_total_loss_te, history_RCE_mol_loss_te, history_RCE_prop_loss_te, history_KLD_loss_te = [], [], [], []\n\n    beta = 0\n    current_step = 0\n    for epoch in range(opt.epochs):\n        total_loss, RCE_mol_loss, RCE_prop_loss, KLD_loss= 0, 0, 0, 0\n        total_loss_te, RCE_mol_loss_te, RCE_prop_loss_te, KLD_loss_te = 0, 0, 0, 0\n        total_loss_accum_te, RCE_mol_loss_accum_te, RCE_prop_loss_accum_te, KLD_loss_accum_te = 0, 0, 0, 0\n        accum_train_printevery_n, accum_test_n, accum_test_printevery_n = 0, 0, 0\n\n        if opt.floyd is False:\n            print(\"     {TR}   %dm: epoch %d [%s]  %d%%  loss = %s\" %\\\n            ((time.time() - start)//60, epoch + 1, \"\".join(' '*20), 0, '...'), end='\\r')\n\n        if opt.checkpoint > 0:\n            torch.save(model.state_dict(), 'weights/model_weights')\n\n        # KL annealing\n        if opt.use_KLA == True:\n            if epoch + 1 >= opt.KLA_beg_epoch and beta < opt.KLA_max_beta:\n                beta = KLAnnealer(opt, epoch)\n        else:\n            beta = 1\n\n        for i, batch in enumerate(opt.train):\n            current_step += 1\n            src = batch.src.transpose(0, 1)#.to('cuda')\n            trg = batch.trg.transpose(0, 1)#.to('cuda')\n            trg_input = trg[:, :-1]\n\n            cond = torch.stack([batch.logP, batch.tPSA, batch.QED]).transpose(0, 1)#.to('cuda')\n\n            src_mask, trg_mask = create_masks(src, trg_input, cond, opt)\n            preds_prop, preds_mol, mu, log_var, z = model(src, trg_input, cond, src_mask, trg_mask)\n            ys_mol = trg[:, 1:].contiguous().view(-1)\n            ys_cond = torch.unsqueeze(cond, 2).contiguous().view(-1, opt.cond_dim, 1)\n\n            opt.optimizer.zero_grad()\n\n            loss, RCE_mol, RCE_prop, KLD = loss_function(opt, beta, preds_prop, preds_mol, ys_cond, ys_mol, mu, log_var)\n\n            loss.backward()\n            opt.optimizer.step()\n            if opt.lr_scheduler == \"SGDR\":\n                opt.sched.step()\n\n            if opt.lr_scheduler == \"WarmUpDefault\":\n                head = np.float(np.power(np.float(current_step), -0.5))\n                tail = np.float(current_step) * np.power(np.float(opt.lr_WarmUpSteps), -1.5)\n                lr = np.float(np.power(np.float(opt.d_model), -0.5)) * min(head, tail)\n                for param_group in opt.optimizer.param_groups:\n                    param_group['lr'] = lr\n\n            for param_group in opt.optimizer.param_groups:\n                current_lr = param_group['lr']\n\n            total_loss += loss.item()\n            RCE_mol_loss += RCE_mol.item()\n            RCE_prop_loss += RCE_prop.item()\n            KLD_loss += KLD.item()\n\n            accum_train_printevery_n += len(batch)\n            if (i + 1) % opt.printevery == 0:\n                 p = int(100 * (i + 1) / opt.train_len)\n                 avg_loss = total_loss /accum_train_printevery_n\n                 avg_RCE_mol_loss = RCE_mol_loss /accum_train_printevery_n\n                 avg_RCE_prop_loss = RCE_prop_loss /accum_train_printevery_n\n                 avg_KLD_loss = KLD_loss /accum_train_printevery_n\n                 if (i + 1) % (opt.historyevery) == 0:\n                     history_epoch.append(epoch + 1)\n                     history_beta.append(beta)\n                     for param_group in opt.optimizer.param_groups:\n                        history_lr.append(param_group['lr'])\n                     history_total_loss.append(avg_loss)\n                     history_RCE_mol_loss.append(avg_RCE_mol_loss)\n                     history_RCE_prop_loss.append(avg_RCE_prop_loss)\n                     history_KLD_loss.append(avg_KLD_loss)\n\n                 if opt.floyd is False:\n                    print(\"     {TR}   %dm: epoch %d [%s%s]  %d%%  loss = %.3f, RCE_mol = %.3f, RCE_prop = %.5f, KLD = %.5f, beta = %.4f, lr = %.6f\" % ((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_loss, avg_RCE_mol_loss, avg_RCE_prop_loss, avg_KLD_loss, beta, current_lr), end='\\r')\n                 else:\n                    print(\"     {TR}   %dm: epoch %d [%s%s]  %d%%  loss = %.3f, RCE_mol = %.3f, RCE_prop = %.5f, KLD = %.5f, beta = %.4f, lr = %.6f\" %\\\n                    ((time.time() - start)//60, epoch + 1, \"\".join('#'*(p//5)), \"\".join(' '*(20-(p//5))), p, avg_loss, avg_RCE_mol_loss, avg_RCE_prop_loss, avg_KLD_loss, beta, current_lr))\n                 accum_train_printevery_n, total_loss, RCE_mol_loss, RCE_prop_loss, KLD_loss = 0, 0, 0, 0, 0\n            \n            if opt.checkpoint > 0 and ((time.time()-cptime)//60) // opt.checkpoint >= 1:\n                torch.save(model.state_dict(), 'weights/model_weights')\n                cptime = time.time()\n\n        print(\"     {TR}   %dm: epoch %d [%s%s]  %d%%  loss = %.3f, RCE_mol = %.3f, RCE_prop = %.5f, KLD = %.5f, beta = %.4f, lr = %.6f\" %\\\n        ((time.time() - start)//60, epoch + 1, \"\".join('#'*(100//5)), \"\".join(' '*(20-(100//5))), 100, avg_loss, avg_RCE_mol_loss, avg_RCE_prop_loss, avg_KLD_loss, beta, current_lr))\n\n\n        # Test\n        if opt.imp_test == True:\n            model.eval()\n\n            if opt.floyd is False:\n                print(\"     {TE}   %dm:         [%s]  %d%%  loss = %s\" %\\\n                ((time.time() - start)//60, \"\".join(' '*20), 0, '...'), end='\\r')\n\n            with torch.no_grad():\n                for i, batch in enumerate(opt.test):\n                    src = batch.src.transpose(0, 1)#.to('cuda')\n                    trg = batch.trg.transpose(0, 1)#.to('cuda')\n                    trg_input = trg[:, :-1]\n                    cond = torch.stack([batch.logP, batch.tPSA, batch.QED]).transpose(0, 1)#.to('cuda')\n\n                    src_mask, trg_mask = create_masks(src, trg_input, cond, opt)\n                    preds_prop, preds_mol, mu, log_var, z = model(src, trg_input, cond, src_mask, trg_mask)\n                    ys_mol = trg[:, 1:].contiguous().view(-1)\n                    ys_cond = torch.unsqueeze(cond, 2).contiguous().view(-1, opt.cond_dim, 1)\n\n                    loss_te, RCE_mol_te, RCE_prop_te, KLD_te = loss_function(opt, beta, preds_prop, preds_mol, ys_cond, ys_mol, mu, log_var)\n\n                    total_loss_te += loss_te.item()\n                    RCE_mol_loss_te += RCE_mol_te.item()\n                    RCE_prop_loss_te += RCE_prop_te.item()\n                    KLD_loss_te += KLD_te.item()\n                    total_loss_accum_te += loss_te.item()\n                    RCE_mol_loss_accum_te += RCE_mol_te.item()\n                    RCE_prop_loss_accum_te += RCE_prop_te.item()\n                    KLD_loss_accum_te += KLD_te.item()\n\n                    accum_test_n += len(batch)\n                    accum_test_printevery_n += len(batch)\n                    if (i + 1) % opt.printevery == 0:\n                        p = int(100 * (i + 1) / opt.test_len)\n                        avg_loss_te = total_loss_te /accum_test_printevery_n\n                        avg_RCE_mol_loss_te = RCE_mol_loss_te /accum_test_printevery_n\n                        avg_RCE_prop_loss_te = RCE_prop_loss_te /accum_test_printevery_n\n                        avg_KLD_loss_te = KLD_loss_te /accum_test_printevery_n\n\n                        if opt.floyd is False:\n                            print(\"     {TE}   %dm:         [%s%s]  %d%%  loss = %.3f, RCE_mol = %.3f, RCE_prop = %.5f, KLD = %.5f, beta = %.4f\" % \\\n                            ((time.time() - start) // 60, \"\".join('#' * (p // 5)), \"\".join(' ' * (20 - (p // 5))), p, avg_loss_te, avg_RCE_mol_loss_te, avg_RCE_prop_loss_te, avg_KLD_loss_te, beta), end='\\r')\n                        else:\n                            print(\"     {TE}   %dm:         [%s%s]  %d%%  loss = %.3f, RCE_mol = %.3f, RCE_prop = %.3f, KLD = %.5f, beta = %.4f\" % \\\n                            ((time.time() - start) // 60, \"\".join('#' * (p // 5)), \"\".join(' ' * (20 - (p // 5))), p, avg_loss_te, avg_RCE_mol_loss_te, avg_RCE_prop_loss_te, avg_KLD_loss_te, beta))\n                        accum_test_printevery_n, total_loss_te, RCE_mol_loss_te, RCE_prop_loss_te, KLD_loss_te = 0, 0, 0, 0, 0\n\n                print(\"     {TE}   %dm:         [%s%s]  %d%%  loss = %.3f, RCE_mol = %.3f, RCE_prop = %.5f, KLD = %.5f, beta = %.4f\\n\" % \\\n                            ((time.time() - start) // 60, \"\".join('#' * (100 // 5)), \"\".join(' ' * (20 - (100 // 5))), 100, avg_loss_te, avg_RCE_mol_loss_te, avg_RCE_prop_loss_te, avg_KLD_loss_te, beta))\n\n            if epoch == 0:\n                opt.margin = len(history_epoch)\n\n            for j in range(opt.margin):\n                history_total_loss_te.append(\"\")\n                history_RCE_mol_loss_te.append(\"\")\n                history_RCE_prop_loss_te.append(\"\")\n                history_KLD_loss_te.append(\"\")\n            history_epoch.append(epoch+1)\n            history_lr.append(current_lr)\n            history_beta.append(beta)\n            history_total_loss_te.append(total_loss_accum_te/len(opt.test.dataset))\n            history_RCE_mol_loss_te.append(RCE_mol_loss_accum_te/len(opt.test.dataset))\n            history_RCE_prop_loss_te.append(RCE_prop_loss_accum_te/len(opt.test.dataset))\n            history_KLD_loss_te.append(KLD_loss_accum_te/len(opt.test.dataset))\n        history_total_loss.append(avg_loss)\n        history_RCE_mol_loss.append(avg_RCE_mol_loss)\n        history_RCE_prop_loss.append(avg_RCE_prop_loss)\n        history_KLD_loss.append(avg_KLD_loss)\n\n        # Export train/test history\n        if opt.imp_test == True:\n            history = pd.DataFrame(\n                {\"epochs\": history_epoch, \"beta\": history_beta, \"lr\": history_lr, \"total_loss\": history_total_loss, \"total_loss_te\": history_total_loss_te,\n                 \"RCE_mol_loss\": history_RCE_mol_loss, \"RCE_mol_loss_te\": history_RCE_mol_loss_te,\n                 \"RCE_prop_loss\": history_RCE_prop_loss, \"RCE_prop_loss_te\": history_RCE_prop_loss_te,\n                 \"KLD_loss\": history_KLD_loss, \"KLD_loss_te\": history_KLD_loss_te})\n            history.to_csv('trHist_lat={}_epo={}_{}.csv'.format(opt.latent_dim, opt.epochs, time.strftime(\"%Y%m%d\")), index=True)\n        else:\n            history = pd.DataFrame(\n                {\"epochs\": history_epoch, \"beta\": history_beta, \"lr\": history_lr, \"total_loss\": history_total_loss, \"RCE_mol_loss\": history_RCE_mol_loss,\n                 \"RCE_prop_loss\": history_RCE_prop_loss, \"KLD_loss\": history_KLD_loss})\n            history.to_csv('trHist_lat={}_epo={}_{}.csv'.format(opt.latent_dim, opt.epochs, time.strftime(\"%Y%m%d\")), index=True)\n\n        # Export weights every epoch\n        # if not os.path.isdir('{}'.format(opt.save_folder_name)):\n        #     os.mkdir('{}'.format(opt.save_folder_name))\n        # if not os.path.isdir('{}/epo{}'.format(opt.save_folder_name, epoch + 1)):\n        #     os.mkdir('{}/epo{}'.format(opt.save_folder_name, epoch + 1))\n        # torch.save(model.state_dict(), f'{opt.save_folder_name}/epo{epoch+1}/model_weights')\n        # joblib.dump(robustScaler, f'{opt.save_folder_name}/epo{epoch+1}/scaler.pkl')\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    # Data settings\n    parser.add_argument('-f')\n    parser.add_argument('-imp_test', type=bool, default=True)\n    parser.add_argument('-src_data', type=str, default='/kaggle/input/train11/train1111.txt')\n    parser.add_argument('-src_data_te', type=str, default='/kaggle/input/testttttt/test11111.txt')\n    parser.add_argument('-trg_data', type=str, default='/kaggle/input/train11/train1111.txt')\n    parser.add_argument('-trg_data_te', type=str, default='/kaggle/input/testttttt/test11111.txt')\n    parser.add_argument('-lang_format', type=str, default='SMILES')\n    parser.add_argument('-calProp', type=bool, default=True) #if prop_temp.csv and prop_temp_te.csv exist, set False\n\n    # Learning hyperparameters\n    parser.add_argument('-epochs', type=int, default=25)\n    parser.add_argument('-no_cuda', type=str, default=False)\n    # parser.add_argument('-lr_scheduler', type=str, default=\"SGDR\", help=\"WarmUpDefault, SGDR\")\n    parser.add_argument('-lr_scheduler', type=str, default=\"WarmUpDefault\", help=\"WarmUpDefault, SGDR\")\n    parser.add_argument('-lr_WarmUpSteps', type=int, default=8000, help=\"only for WarmUpDefault\")\n    parser.add_argument('-lr', type=float, default=0.0001)\n    parser.add_argument('-lr_beta1', type=float, default=0.9)\n    parser.add_argument('-lr_beta2', type=float, default=0.98)\n    parser.add_argument('-lr_eps', type=float, default=1e-9)\n\n    # KL Annealing\n    parser.add_argument('-use_KLA', type=bool, default=True)\n    parser.add_argument('-KLA_ini_beta', type=float, default=0.02)\n    parser.add_argument('-KLA_inc_beta', type=float, default=0.02)\n    parser.add_argument('-KLA_max_beta', type=float, default=1.0)\n    parser.add_argument('-KLA_beg_epoch', type=int, default=1) #KL annealing begin\n\n    # Network sturucture\n    parser.add_argument('-use_cond2dec', type=bool, default=False)\n    parser.add_argument('-use_cond2lat', type=bool, default=True)\n    parser.add_argument('-latent_dim', type=int, default=128)\n    parser.add_argument('-cond_dim', type=int, default=3)\n    parser.add_argument('-d_model', type=int, default=512)\n    parser.add_argument('-n_layers', type=int, default=6)\n    parser.add_argument('-heads', type=int, default=8)\n    parser.add_argument('-dropout', type=int, default=0.3)\n    parser.add_argument('-batchsize', type=int, default=256)\n    # parser.add_argument('-batchsize', type=int, default=1024*8)\n    parser.add_argument('-max_strlen', type=int, default=80)  # max 80\n\n    # History\n    parser.add_argument('-verbose', type=bool, default=False)\n    parser.add_argument('-save_folder_name', type=str, default='saved_model')\n    parser.add_argument('-print_model', type=bool, default=False)\n    parser.add_argument('-printevery', type=int, default=5)\n    parser.add_argument('-historyevery', type=int, default=5) # must be a multiple of printevery\n    parser.add_argument('-load_weights')\n    parser.add_argument('-create_valset', action='store_true')\n    parser.add_argument('-floyd', action='store_true')\n    parser.add_argument('-checkpoint', type=int, default=0)\n\n    opt = parser.parse_args()\n    opt.device = 0 if opt.no_cuda is False else -1\n\n    if opt.historyevery % opt.printevery != 0:\n        raise ValueError(\"historyevery must be a multiple of printevery: {} % {} != 0\".format(opt.historyevery, opt.printevery))\n\n    if opt.device == 0:\n        assert torch.cuda.is_available()\n    \n    read_data(opt)\n\n    # Property calculation: logP, tPSA, QED\n    if opt.calProp == True:\n        PROP, PROP_te = calcProperty(opt)\n    else:\n        PROP, PROP_te = pd.read_csv(\"/kaggle/working/prop_temp.csv\"), pd.read_csv(\"/kaggle/working/prop_temp_te.csv\")\n\n    SRC, TRG = create_fields(opt)\n    opt.max_logP, opt.min_logP, opt.max_tPSA, opt.min_tPSA, opt.max_QED, opt.min_QED \\\n        = PROP[\"logP\"].max(), PROP[\"logP\"].min(), PROP[\"tPSA\"].max(), PROP[\"tPSA\"].min(), PROP_te[\"QED\"].max(), PROP_te[\"QED\"].min()\n\n    robustScaler = RobustScaler()\n    robustScaler.fit(PROP)\n    # if not os.path.isdir('{}'.format(opt.save_folder_name)):\n    #     os.mkdir('{}'.format(opt.save_folder_name))\n    # joblib.dump(robustScaler, 'scaler.pkl')\n    # robustScaler = joblib.load('scaler.pkl')\n\n    PROP, PROP_te = pd.DataFrame(robustScaler.transform(PROP)), pd.DataFrame(robustScaler.transform(PROP_te))\n\n    opt.train = create_dataset(opt, SRC, TRG, PROP, tr_te='tr')\n    opt.test = create_dataset(opt, SRC, TRG, PROP_te, tr_te='te')\n\n    model = get_model(opt, len(SRC.vocab), len(TRG.vocab))\n    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(\"# of trainable parameters: {}\".format(total_trainable_params))\n\n\n    opt.optimizer = torch.optim.Adam(model.parameters(), lr=opt.lr, betas=(opt.lr_beta1, opt.lr_beta2), eps=opt.lr_eps)\n    if opt.lr_scheduler == \"SGDR\":\n        opt.sched = CosineWithRestarts(opt.optimizer, T_max=opt.train_len)\n\n    if opt.checkpoint > 0:\n        print(\"model weights will be saved every %d minutes and at end of epoch to directory weights/\"%(opt.checkpoint))\n    \n    if opt.load_weights is not None and opt.floyd is not None:\n        os.mkdir('weights')\n        pickle.dump(SRC, open('weights/SRC.pkl', 'wb'))\n        pickle.dump(TRG, open('weights/TRG.pkl', 'wb'))\n    \n    train_model(model, opt)\n\n    if opt.floyd is False:\n        promptNextAction(model, opt, SRC, TRG, robustScaler)\n\ndef yesno(response):\n    while True:\n        if response != 'y' and response != 'n':\n            response = input('command not recognised, enter y or n : ')\n        else:\n            return response\n\ndef promptNextAction(model, opt, SRC, TRG, robustScaler):\n\n    saved_once = 1 if opt.load_weights is not None or opt.checkpoint > 0 else 0\n    \n    if opt.load_weights is not None:\n        dst = opt.load_weights\n    if opt.checkpoint > 0:\n        dst = 'weights'\n\n    while True:\n        save = yesno(input('training complete, save results? [y/n] : '))\n        if save == 'y':\n            while True:\n                if saved_once != 0:\n                    res = yesno(\"save to same folder? [y/n] : \")\n                    if res == 'y':\n                        break\n                dst = input('enter folder name to create for weights (no spaces) : ')\n                if ' ' in dst or len(dst) < 1 or len(dst) > 30:\n                    dst = input(\"name must not contain spaces and be between 1 and 30 characters length, enter again : \")\n                else:\n                    try:\n                        os.mkdir(dst)\n                    except:\n                        res= yesno(input(dst + \" already exists, use anyway? [y/n] : \"))\n                        if res == 'n':\n                            continue\n                    break\n            \n            print(\"saving weights to \" + dst + \"/...\")\n            torch.save(model.state_dict(), f'{dst}/model_weights')\n            if saved_once == 0:\n                pickle.dump(SRC, open(f'{dst}/SRC.pkl', 'wb'))\n                pickle.dump(TRG, open(f'{dst}/TRG.pkl', 'wb'))\n                joblib.dump(robustScaler, open(f'{dst}/scaler.pkl', 'wb'))\n                saved_once = 1\n            \n            print(\"weights and field pickles saved to \" + dst)\n\n        res = yesno(input(\"train for more epochs? [y/n] : \"))\n        if res == 'y':\n            while True:\n                epochs = input(\"type number of epochs to train for : \")\n                try:\n                    epochs = int(epochs)\n                except:\n                    print(\"input not a number\")\n                    continue\n                if epochs < 1:\n                    print(\"epochs must be at least 1\")\n                    continue\n                else:\n                    break\n            opt.epochs = epochs\n            train_model(model, opt)\n        else:\n            print(\"exiting program...\")\n            break\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2023-05-01T12:25:26.214273Z","iopub.execute_input":"2023-05-01T12:25:26.214702Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Calculating properties for 15919 train molecules: logP, tPSA, QED\n  [####################] 100%  completed!Calculating properties for 2039 test molecules: logP, tPSA, QED\n  [####################] 100%  completed!loading molecule tokenizers...\n\n* creating [train] dataset and iterator... \n     - # of training samples: 15169\n     - building vocab from train data...\n\n* creating [test] dataset and iterator... \n     - # of test samples: 1869\n# of trainable parameters: 44447552\ntraining model...\n     {TR}   0m: epoch 1 [                    ]  0%  loss = ...\r","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:93: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:94: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:95: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","output_type":"stream"},{"name":"stdout","text":"     {TR}   28m: epoch 1 [####################]  100%  loss = 86.845, RCE_mol = 81.491, RCE_prop = 0.00000, KLD = 267.70651, beta = 0.0200, lr = 0.0000953\n     {TE}   30m:         [####################]  100%  loss = 76.557, RCE_mol = 75.810, RCE_prop = 0.00000, KLD = 37.34416, beta = 0.0200\n\n     {TR}   57m: epoch 2 [####################]  100%  loss = 48.752, RCE_mol = 47.970, RCE_prop = 0.00000, KLD = 19.55073, beta = 0.0400, lr = 0.000190\n     {TE}   59m:         [####################]  100%  loss = 53.783, RCE_mol = 53.128, RCE_prop = 0.00000, KLD = 16.36691, beta = 0.0400\n\n     {TR}   85m: epoch 3 [####################]  100%  loss = 2.576, RCE_mol = 2.495, RCE_prop = 0.00000, KLD = 1.34893, beta = 0.0600, lr = 0.00028584\n     {TE}   87m:         [####################]  100%  loss = 57.218, RCE_mol = 55.924, RCE_prop = 0.00000, KLD = 21.56483, beta = 0.0600\n\n     {TR}   114m: epoch 4 [####################]  100%  loss = 43.387, RCE_mol = 41.983, RCE_prop = 0.00000, KLD = 17.55350, beta = 0.0800, lr = 0.000380\n     {TE}   116m:         [####################]  100%  loss = 53.409, RCE_mol = 52.086, RCE_prop = 0.00000, KLD = 16.54087, beta = 0.0800\n\n     {TR}   142m: epoch 5 [####################]  100%  loss = 41.355, RCE_mol = 39.947, RCE_prop = 0.00000, KLD = 14.07954, beta = 0.1000, lr = 0.000475\n     {TE}   144m:         [####################]  100%  loss = 54.619, RCE_mol = 53.310, RCE_prop = 0.00000, KLD = 13.09144, beta = 0.1000\n\n     {TR}   171m: epoch 6 [####################]  100%  loss = 36.338, RCE_mol = 35.038, RCE_prop = 0.00000, KLD = 10.83542, beta = 0.1200, lr = 0.000460\n     {TE}   173m:         [####################]  100%  loss = 57.464, RCE_mol = 56.432, RCE_prop = 0.00000, KLD = 8.60003, beta = 0.1200\n\n     {TR}   187m: epoch 7 [#########           ]  49%  loss = 30.835, RCE_mol = 29.439, RCE_prop = 0.00000, KLD = 9.96903, beta = 0.1400, lr = 0.0004422\r","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score\n\ndef evaluate(model, opt, dataloader):\n    model.eval()\n    true_labels, pred_labels = [], []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            src = batch.src.transpose(0, 1)\n            trg = batch.trg.transpose(0, 1)\n            trg_input = trg[:, :-1]\n            cond = torch.stack([batch.logP, batch.tPSA, batch.QED]).transpose(0, 1)\n            src_mask, trg_mask = create_masks(src, trg_input, cond, opt)\n            preds_prop, preds_mol, mu, log_var, z = model(src, trg_input, cond, src_mask, trg_mask)\n            ys_mol = trg[:, 1:].contiguous().view(-1)\n            ys_cond = torch.unsqueeze(cond, 2).contiguous().view(-1, opt.cond_dim, 1)\n            _, predicted = torch.max(preds_mol.data, 1)\n            true_labels.extend(ys_mol.cpu().numpy().tolist())\n            pred_labels.extend(predicted.cpu().numpy().tolist())\n\n    f1 = f1_score(true_labels, pred_labels)\n    acc = accuracy_score(true_labels, pred_labels)\n\n    return f1, acc\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data distribution\nimport matplotlib.pyplot as plt\nimport matplotlib.patheffects as path_effects\nimport seaborn as sns\nimport pandas as pd\n\ndef checkdata(fpath):\n    # fpath = \"data/moses/prop_temp.csv\"\n    results = pd.read_csv(fpath)\n\n    logP, tPSA, QED = results.iloc[:, 0], results.iloc[:, 1], results.iloc[:, 2]\n\n    figure, ((ax1,ax2,ax3)) = plt.subplots(nrows=1, ncols=3)\n\n    sns.violinplot(y = \"logP\", data =results, ax=ax1, color=sns.color_palette()[0])\n    sns.violinplot(y = \"tPSA\", data =results, ax=ax2, color=sns.color_palette()[1])\n    sns.violinplot(y = \"QED\", data =results, ax=ax3, color=sns.color_palette()[2])\n\n    ax1.set(xlabel='logP', ylabel='')\n    ax2.set(xlabel='tPSA', ylabel='')\n    ax3.set(xlabel='QED', ylabel='')\n\n    bound_logP = get_quatiles(logP)\n    for i in range(4):\n        text = ax1.text(0, bound_logP[i], f'{bound_logP[i]:.2f}', ha='right', va='center', fontweight='bold', size=10, color='white')\n        text.set_path_effects([path_effects.Stroke(linewidth=3, foreground='black'), path_effects.Normal(), ])\n\n    bound_tPSA = get_quatiles(tPSA)\n    for i in range(4):\n        text = ax2.text(0, bound_tPSA[i], f'{bound_tPSA[i]:.2f}', ha='right', va='center', fontweight='bold', size=10, color='white')\n        text.set_path_effects([path_effects.Stroke(linewidth=3, foreground='black'), path_effects.Normal(), ])\n\n    bound_QED = get_quatiles(QED)\n    for i in range(4):\n        text = ax3.text(0, bound_QED[i], f'{bound_QED[i]:.2f}', ha='right', va='center', fontweight='bold', size=10, color='white')\n        text.set_path_effects([path_effects.Stroke(linewidth=3, foreground='black'), path_effects.Normal(), ])\n\n    plt.show()\n\n    logP_max, logP_min = min(bound_logP[0], logP.max()), max(bound_logP[-1], logP.min())\n    tPSA_max, tPSA_min = min(bound_tPSA[0], tPSA.max()), max(bound_tPSA[-1], tPSA.min())\n    QED_max, QED_min = min(bound_QED[0], QED.max()), max(bound_QED[-1], QED.min())\n\n    return logP_max, logP_min, tPSA_max, tPSA_min, QED_max, QED_min\n\n\ndef get_quatiles(df):\n    Q1 = df.quantile(0.25)\n    Q3 = df.quantile(0.75)\n    IQR = Q3 - Q1\n    UAV = Q3 + 1.5 * IQR\n    LAV = Q1 - 1.5 * IQR\n    return [UAV, Q3, Q1, LAV]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#randem gene\n# Adjusted from https://alpynepyano.github.io/healthyNumerics/posts/sampling_arbitrary_distributions_with_python.html\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef plot_distrib1(xc, count_c):\n    with plt.style.context('fivethirtyeight'):\n        plt.figure(figsize=(17,5))\n        plt.plot(xc,count_c, ls='--', lw=1, c='b')\n        wi = np.diff(xc)[0]*0.95\n        plt.bar (xc, count_c, color='gold', width=wi, alpha=0.7, label='Histogram of data')\n        plt.title('Data distribution of tokenlen', fontsize=25, fontweight='bold')\n        plt.show()\n    return\n\ndef plot_line(X,Y,x,y):\n    with plt.style.context('fivethirtyeight'):\n        fig, ax1 = plt.subplots(figsize=(17,5))\n        ax1.plot(X,Y, 'mo-', lw=7, label='discrete CDF', ms=20)\n        ax1.legend(loc=6, frameon=False)\n        ax2 = ax1.twinx()\n        ax2.plot(x,y, 'co-', lw=7, label='discrete PDF', ms=20)\n        ax2.legend(loc=7, frameon=False)\n        ax1.set_ylabel('CDF-axis');  ax2.set_ylabel('PDF-axis');\n        plt.title('Tokenlen: CDF and PDF', fontsize=25, fontweight='bold')\n        plt.show()\n\ndef plot_distrib3(xc, myPDF, X):\n    with plt.style.context('fivethirtyeight'):\n        plt.figure(figsize=(17,5))\n        width, ms = 0.5, 20\n        plt.bar(xc, X, color='blue', width=width, label='resampled PDF')\n        plt.plot(xc, np.zeros_like(X) ,color='magenta', ls='-',lw=13, alpha=0.6)\n        plt.plot(xc, myPDF, 'co-', lw=7, label='discrete PDF', ms=ms, alpha=0.5)\n        plt.title('Tokenlen sampling from data distribution', fontsize=25, fontweight='bold')\n        plt.legend(loc='upper center', frameon=False)\n        plt.show()\n\ndef get_sampled_element(myCDF):\n    a = np.random.uniform(0, 1)\n    return np.argmax(myCDF>=a)-1\n\ndef run_sampling(xc, dxc, myPDF, myCDF, nRuns):\n    sample_list = []\n    X = np.zeros_like(myPDF, dtype=int)\n    for k in np.arange(nRuns):\n        idx = get_sampled_element(myCDF)\n        sample_list.append(xc[idx] + dxc * np.random.normal() / 2)\n        X[idx] += 1\n    return np.array(sample_list).reshape(nRuns, 1), X/np.sum(X)\n\ndef tokenlen_gen_from_data_distribution(data, nBins, size):\n    count_c, bins_c, = np.histogram(data, bins=nBins)\n    myPDF = count_c / np.sum(count_c)\n    dxc = np.diff(bins_c)[0]\n    xc = bins_c[0:-1] + 0.5 * dxc\n\n    myCDF = np.zeros_like(bins_c)\n    myCDF[1:] = np.cumsum(myPDF)\n\n    tokenlen_list, X = run_sampling(xc, dxc, myPDF, myCDF, size)\n\n    # plot_distrib1(xc, myPDF)\n    # plot_line(bins_c, myCDF, xc, myPDF)\n    # plot_distrib3(xc, myPDF, X)\n\n    return tokenlen_list\n\ndef rand_gen_from_data_distribution(data, size, nBins):\n    H, edges = np.histogramdd(data.values, bins=(nBins[0], nBins[1], nBins[2]))\n    P = H/len(data)\n    P_flatten = P.reshape(-1)\n\n    dxc_logP, dxc_tPSA, dxc_QED = np.diff(edges[0])[0], np.diff(edges[1])[0], np.diff(edges[2])[0]\n    xc_logP, xc_tPSA, xc_QED = edges[0][0:-1] + 0.5 * dxc_logP, edges[1][0:-1] + 0.5 * dxc_tPSA, edges[2][0:-1] + 0.5 * dxc_QED\n\n    samples_idx = np.random.choice(len(P_flatten), size=size, p=P_flatten)\n    samples_idx = np.array(np.unravel_index(samples_idx, P.shape)).T\n\n    samples = np.zeros_like(samples_idx, dtype=np.float64)\n\n    for i in range(len(samples_idx)):\n        samples[i] = [xc_logP[samples_idx[i][0]], xc_tPSA[samples_idx[i][1]], xc_QED[samples_idx[i][2]]]\n\n    random_noise = np.random.uniform(low=-0.5, high=0.5, size=np.shape(samples))\n    random_noise[:, 0] = random_noise[:, 0] * dxc_logP\n    random_noise[:, 1] = random_noise[:, 1] * dxc_tPSA\n    random_noise[:, 2] = random_noise[:, 2] * dxc_QED\n\n    samples = samples + random_noise\n\n    return samples\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nfrom io import StringIO\nimport argparse\nimport time\nimport torch\n#from Models import get_model\n#from Process import *\nimport torch.nn.functional as F\n#from Optim import CosineWithRestarts\n#from Batch import create_masks\nimport pdb\n#import dill as pickle\nimport argparse\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors, QED, rdDepictor, AllChem, Draw\nfrom rdkit.Chem.Draw import rdMolDraw2D\n#from Models import get_model\n#from Beam import beam_search\nfrom nltk.corpus import wordnet\nfrom torch.autograd import Variable\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nimport joblib\nimport re\nimport numpy as np\nimport math\n#import moses\n#from rand_gen import rand_gen_from_data_distribution, tokenlen_gen_from_data_distribution\n#from dataDistibutionCheck import checkdata\n\ndef get_synonym(word, SRC):\n    syns = wordnet.synsets(word)\n    for s in syns:\n        for l in s.lemmas():\n            if SRC.vocab.stoi[l.name()] != 0:\n                return SRC.vocab.stoi[l.name()]\n            \n    return 0\n\ndef gen_mol(cond, model, opt, SRC, TRG, toklen, z):\n    model.eval()\n\n    robustScaler = joblib.load(opt.load_weights + '/scaler.pkl')\n    if opt.conds == 'm':\n        cond = cond.reshape(1, -1)\n    elif opt.conds == 's':\n        cond = cond.reshape(1, -1)\n    elif opt.conds == 'l':\n        cond = cond.reshape(1, -1)\n    else:\n        cond = np.array(cond.split(',')[:-1]).reshape(1, -1)\n\n    cond = robustScaler.transform(cond)\n    cond = Variable(torch.Tensor(cond))\n    \n    sentence = beam_search(cond, model, SRC, TRG, toklen, opt, z)\n    return sentence\n\n\ndef inference(opt, model, SRC, TRG):\n    molecules, val_check, conds_trg, conds_rdkit, toklen_check, toklen_gen = [], [], [], [], [], []\n    if opt.conds == 'm':\n        print(\"\\nGenerating molecules for MOSES benchmarking...\")\n        n_samples = 30000\n        nBins = [1000, 1000, 1000]\n\n        data = pd.read_csv(opt.load_traindata)\n        toklen_data = pd.read_csv(opt.load_toklendata)\n\n        conds = rand_gen_from_data_distribution(data, size=n_samples, nBins=nBins)\n        toklen_data = tokenlen_gen_from_data_distribution(data=toklen_data, nBins=int(toklen_data.max()-toklen_data.min()), size=n_samples)\n\n        start = time.time()\n        for idx in range(n_samples):\n            toklen = int(toklen_data[idx]) + 3  # +3 due to cond2enc\n            z = torch.Tensor(np.random.normal(size=(1, toklen, opt.latent_dim)))\n            molecule_tmp = gen_mol(conds[idx], model, opt, SRC, TRG, toklen, z)\n            toklen_gen.append(molecule_tmp.count(' ')+1)\n            molecule_tmp = ''.join(molecule_tmp).replace(\" \", \"\")\n\n            molecules.append(molecule_tmp)\n            conds_trg.append(conds[idx])\n            # toklen-3: due to cond dim\n            toklen_check.append(toklen-3)\n            m = Chem.MolFromSmiles(molecule_tmp)\n            if m is None:\n                val_check.append(0)\n                conds_rdkit.append([None, None, None])\n            else:\n                val_check.append(1)\n                conds_rdkit.append(np.array([Descriptors.MolLogP(m), Descriptors.TPSA(m), QED.qed(m)]))\n\n            if (idx+1) % 100 == 0:\n                print(\"*   {}m: {} / {}\".format((time.time() - start)//60, idx+1, n_samples))\n\n            if (idx+1) % 2000 == 0:\n                np_conds_trg, np_conds_rdkit = np.array(conds_trg), np.array(conds_rdkit)\n                gen_list = pd.DataFrame(\n                    {\"mol\": molecules, \"val_check\": val_check, \"trg(logP)\": np_conds_trg[:, 0], \"trg(tPSA)\": np_conds_trg[:, 1], \"trg(QED)\": np_conds_trg[:, 2], \"rdkit(logP)\": np_conds_rdkit[:, 0], \"rdkit(tPSA)\": np_conds_rdkit[:, 1], \"rdkit(QED)\": np_conds_rdkit[:, 2], \"toklen\": toklen_check, \"toklen_gen\": toklen_gen})\n                gen_list.to_csv('moses_bench2_lat={}_epo={}_k={}_{}.csv'.format(opt.latent_dim, opt.epochs, opt.k, time.strftime(\"%Y%m%d\")), index=True)\n\n        print(\"Please check the file: 'moses_bench2_lat={}_epo={}_k={}_{}.csv'\".format(opt.latent_dim, opt.epochs, opt.k, time.strftime(\"%Y%m%d\")))\n\n\n    elif opt.conds == 's':\n        print(\"\\nGenerating molecules for 10 condition sets...\")\n        n_samples = 10\n        n_per_samples = 200\n        nBins = [1000, 1000, 1000]\n\n        data = pd.read_csv(opt.load_traindata)\n        toklen_data = pd.read_csv(opt.load_toklendata)\n\n        conds = rand_gen_from_data_distribution(data, size=n_samples, nBins=nBins)\n        toklen_data = tokenlen_gen_from_data_distribution(data=toklen_data, nBins=int(toklen_data.max()-toklen_data.min()), size=n_samples*n_per_samples)\n\n        print(\"conds:\\n\", conds)\n        start = time.time()\n        for idx in range(n_samples):\n            for i in range(n_per_samples):\n                toklen = int(toklen_data[idx*n_per_samples + i]) + 3  # +3 due to cond2enc\n                z = torch.Tensor(np.random.normal(size=(1, toklen, opt.latent_dim)))\n                molecule_tmp = gen_mol(conds[idx], model, opt, SRC, TRG, toklen, z)\n                toklen_gen.append(molecule_tmp.count(\" \") + 1)\n                molecule_tmp = ''.join(molecule_tmp).replace(\" \", \"\")\n\n                molecules.append(molecule_tmp)\n                conds_trg.append(conds[idx])\n\n                toklen_check.append(toklen-3) # toklen -3: due to cond size\n                m = Chem.MolFromSmiles(molecule_tmp)\n                if m is None:\n                    val_check.append(0)\n                    conds_rdkit.append([None, None, None])\n                else:\n                    val_check.append(1)\n                    conds_rdkit.append(np.array([Descriptors.MolLogP(m), Descriptors.TPSA(m), QED.qed(m)]))\n\n                if (idx*n_per_samples+i+1) % 100 == 0:\n                    print(\"*   {}m: {} / {}\".format((time.time() - start)//60, idx*n_per_samples+i+1, n_samples*n_per_samples))\n\n                if (idx*n_per_samples+i+1) % 200 == 0:\n                    np_conds_trg, np_conds_rdkit = np.array(conds_trg), np.array(conds_rdkit)\n                    gen_list = pd.DataFrame(\n                        {\"set_idx\": idx, \"mol\": molecules, \"val_check\": val_check, \"trg(logP)\": np_conds_trg[:, 0], \"trg(tPSA)\": np_conds_trg[:, 1], \"trg(QED)\": np_conds_trg[:, 2], \"rdkit(logP)\": np_conds_rdkit[:, 0], \"rdkit(tPSA)\": np_conds_rdkit[:, 1], \"rdkit(QED)\": np_conds_rdkit[:, 2], \"toklen\": toklen_check, \"toklen_gen\": toklen_gen})\n                    gen_list.to_csv('moses_bench2_10conds_lat={}_epo={}_k={}_{}.csv'.format(opt.latent_dim, opt.epochs, opt.k, time.strftime(\"%Y%m%d\")), index=True)\n\n        print(\"Please check the file: 'moses_bench2_10conds_lat={}_epo={}_k={}_{}.csv'\".format(opt.latent_dim, opt.epochs, opt.k, time.strftime(\"%Y%m%d\")))\n\n    else:\n        conds = opt.conds.split(';')\n        toklen_data = pd.read_csv(opt.load_toklendata)\n        toklen= int(tokenlen_gen_from_data_distribution(data=toklen_data, nBins=int(toklen_data.max() - toklen_data.min()), size=1)) + 3  # +3 due to cond2enc\n\n        z = torch.Tensor(np.random.normal(size=(1, toklen, opt.latent_dim)))\n\n        for cond in conds:\n            molecules.append(gen_mol(cond + ',', model, opt, SRC, TRG, toklen, z))\n        toklen_gen = molecules[0].count(\" \") + 1\n        molecules = ''.join(molecules).replace(\" \", \"\")\n        m = Chem.MolFromSmiles(molecules)\n        target_cond = conds[0].split(',')\n        if m is None:\n            #toklen-3: due to cond dim\n            print(\"   --[Invalid]: {}\".format(molecules))\n            print(\"   --Target: logP={}, tPSA={}, QED={}, LatentToklen={}\".format(target_cond[0], target_cond[1], target_cond[2], toklen-3))\n        else:\n            logP_v, tPSA_v, QED_v = Descriptors.MolLogP(m), Descriptors.TPSA(m), QED.qed(m)\n            print(\"   --[Valid]: {}\".format(molecules))\n            print(\"   --Target: logP={}, tPSA={}, QED={}, LatentToklen={}\".format(target_cond[0], target_cond[1], target_cond[2], toklen-3))\n            print(\"   --From RDKit: logP={:,.4f}, tPSA={:,.4f}, QED={:,.4f}, GenToklen={}\".format(logP_v, tPSA_v, QED_v, toklen_gen))\n\n    return molecules\n\n\ndef main():\n   \n    parser = argparse.ArgumentParser()\n    parser.add_argument('-f')\n    parser.add_argument('-load_weights', type=str, default=\"/content/weight\")\n    parser.add_argument('-load_traindata', type=str, default=\"/content/data/moses/prop_temp.csv\")\n    parser.add_argument('-load_toklendata', type=str, default='/content/toklen_list.csv')\n    parser.add_argument('-k', type=int, default=4)\n    parser.add_argument('-lang_format', type=str, default='SMILES')\n    parser.add_argument('-max_strlen', type=int, default=80) #max 80\n    parser.add_argument('-d_model', type=int, default=512)\n    parser.add_argument('-n_layers', type=int, default=6)\n\n    parser.add_argument('-use_cond2dec', type=bool, default=False)\n    parser.add_argument('-use_cond2lat', type=bool, default=True)\n    parser.add_argument('-cond_dim', type=int, default=3)\n    parser.add_argument('-latent_dim', type=int, default=128)\n\n    parser.add_argument('-epochs', type=int, default=10)\n    parser.add_argument('-lr', type=int, default=0.0001)\n    parser.add_argument('-lr_beta1', type=int, default=0.9)\n    parser.add_argument('-lr_beta2', type=int, default=0.98)\n\n    parser.add_argument('-print_model', type=bool, default=False)\n    parser.add_argument('-heads', type=int, default=8)\n    parser.add_argument('-dropout', type=int, default=0.1)\n    parser.add_argument('-no_cuda', action='store_true')\n    parser.add_argument('-floyd', action='store_true')\n    \n    opt = parser.parse_args()\n\n    opt.device = 0 if opt.no_cuda is False else -1\n\n    assert opt.k > 0\n    assert opt.max_strlen > 10\n\n    SRC, TRG = create_fields(opt)\n    model = get_model(opt, len(SRC.vocab), len(TRG.vocab))\n\n    opt.max_logP, opt.min_logP, opt.max_tPSA, opt.min_tPSA, opt.max_QED, opt.min_QED = checkdata(opt.load_traindata)\n\n    while True:\n        opt.conds =input(\"\\nEnter logP, tPSA, QED to generate molecules (refer the pop-up data distribution)\\\n        \\n* logP: {:.2f} ~ {:.2f}; tPSA: {:.2f} ~ {:.2f}; QED: {:.2f} ~ {:.2f} is recommended.\\\n        \\n* Typing sample: 2.2, 85.3, 0.8\\n* Enter the properties (Or type m: MOSES benchmark, s: 10-Condition set test, q: quit):\".format(opt.min_logP, opt.max_logP, opt.min_tPSA, opt.max_tPSA, opt.min_QED, opt.max_QED))\n\n        if opt.conds==\"q\":\n            break\n        if opt.conds == \"m\":\n            molecule = inference(opt, model, SRC, TRG)\n            break\n        if opt.conds == \"s\":\n            molecule = inference(opt, model, SRC, TRG)\n            break\n        else:\n            molecule = inference(opt, model, SRC, TRG)\n\n\nif __name__ == '__main__':\n    main()","metadata":{},"execution_count":null,"outputs":[]}]}